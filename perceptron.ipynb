{
 "metadata": {
  "name": "",
  "signature": "sha256:8c55c924c7d55527786461984cab7bacb959c7af6abdf2afc77e03f58b8405e6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def prepare_data(data, label_idx, label_a, label_b):\n",
      "    # extract labels 1 and 2 from data\n",
      "    data_label_a = data[data[:,label_idx] == label_a]\n",
      "    data_label_b = data[data[:,label_idx] == label_b]\n",
      "    \n",
      "    # re-label data: label_1 = 0 and label_2 = 1\n",
      "    data_label_a[:,label_idx] = 0\n",
      "    data_label_b[:,label_idx] = 1\n",
      "    \n",
      "    # verticaly stack matrices and randomly shuffle rows\n",
      "    binary_data = np.vstack((data_label_a, data_label_b))\n",
      "    np.random.shuffle(binary_data)\n",
      "    \n",
      "    return binary_data\n",
      "\n",
      "def add_bias(data):\n",
      "    # create bias vector\n",
      "    bias = np.ones((1,data.shape[1]))\n",
      "    \n",
      "    # return verticaly stacked bias on data\n",
      "    return np.vstack((bias, data))\n",
      "\n",
      "def initial_beta(data, label_idx):\n",
      "    # extract labels 0 and 1 from data\n",
      "    data_label_0 = data[data[:,label_idx] == 0][:,1:]\n",
      "    data_label_1 = data[data[:,label_idx] == 1][:,1:]\n",
      "    \n",
      "    # caculate means of the column vectors\n",
      "    mean_0 = np.mean(data_label_0, axis=0)[:,np.newaxis]\n",
      "    mean_1 = np.mean(data_label_1, axis=0)[:,np.newaxis]\n",
      "    \n",
      "    # return the difference of the means for labels 0 and 1\n",
      "    return mean_0 - mean_1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get training data\n",
      "with gzip.open('../Data/zip.train.gz') as f:\n",
      "    train_data = np.loadtxt(f)\n",
      "\n",
      "# get testing data\n",
      "with gzip.open('../Data/zip.test.gz') as f:\n",
      "    test_data = np.loadtxt(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define labeled pair to be used and convert digit labels to binary labels\n",
      "bd = prepare_data(train_data, 0, 2, 3)\n",
      "\n",
      "# extend observations to contain a row of 1's at index 0 (bias)\n",
      "bd_withBias = add_bias(bd)\n",
      "\n",
      "# create initial vector of weights (beta) by subtracting the feature means, of both data sets, from one another\n",
      "beta = initial_beta(bd_withBias, 0)\n",
      "\n",
      "# seperate X and y\n",
      "X = bd_withBias[:,1:]\n",
      "y = bd_withBias[:,0][:,np.newaxis]\n",
      "\n",
      "# set initial step size (gamma)\n",
      "gamma = 0.1\n",
      "\n",
      "# while the difference in the current error and previous error is significant \n",
      "error_flag = True\n",
      "previous_error = 0\n",
      "while error_flag:\n",
      "    \n",
      "    # initialize error, previous error, and error gradient to zeros\n",
      "    error = 0\n",
      "    error_gradient_sum = np.zeros(beta.shape)\n",
      "    \n",
      "    # iterate through each observation\n",
      "    for observation_idx in range(X.shape[0]):\n",
      "    \n",
      "        # get current observation\n",
      "        x = X[observation_idx][:,np.newaxis]\n",
      "    \n",
      "        # caculate y_hat using sigmoid function\n",
      "        y_hat = 1 / (1 + np.exp(np.dot(-beta.T, x)))\n",
      "    \n",
      "        # caculate the error and add to sum of errors\n",
      "        difference = y_hat - y[observation_idx]\n",
      "        error += 0.5 * (difference)**2\n",
      "        \n",
      "        # caculate gradient of error and add it to the sum of gradient errors\n",
      "        error_gradient = (difference * y_hat * (1 - y_hat)) * x\n",
      "        error_gradient_sum = np.sum(np.hstack(( error_gradient_sum, error_gradient)), axis=1)[:,np.newaxis]\n",
      "        \n",
      "    # check if the difference in the current error and previous error is significant\n",
      "    if abs(previous_error - error) < 0.0001:\n",
      "        print error\n",
      "        error_flag = False\n",
      "    \n",
      "    # else update weights (beta)\n",
      "    else:\n",
      "        previous_error = error\n",
      "        beta -= gamma * error_gradient_sum"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 329.00011292]]\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use new weight coeficients to classify test set\n",
      "\n",
      "# output confusion matrix scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}